{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import joblib\n",
    "import copy\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, make_forecasting_frame\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import extract_features, EfficientFCParameters, MinimalFCParameters\n",
    "\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data_feature import *\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=1325, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.decoder = nn.Linear(feature_size,feature_size)\n",
    "        # self.decoder = FeedForward(feature_size,4*feature_size,dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.decoder(x)\n",
    "        return output\n",
    "\n",
    "def get_batch(source, label, i, batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    input = source[i:i + seq_len]\n",
    "    target = label[i:i + seq_len]\n",
    "    return input, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def xgboot_reg(data, target):\n",
    "    # 切分训练集和测试集\n",
    "    data = torch.FloatTensor(data.to_numpy())\n",
    "    target = torch.FloatTensor(target.to_numpy())\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.3, random_state=random.randint(1,10000))\n",
    "\n",
    "    model = TransAm()\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = 0.01\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "    model.train()  # Turn on the train mode\n",
    "    epochs = 20\n",
    "    batch_size = 256\n",
    "\n",
    "    global bst\n",
    "    for epoch in range(epochs):\n",
    "        for batch, i in enumerate(range(0, len(train_x) - 1, batch_size)):\n",
    "            data, targets = get_batch(train_x,train_y, i, batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x).detach().numpy()\n",
    "            dtrain = xgb.DMatrix(output, label=copy.deepcopy(train_y))\n",
    "            # 建模与预测：50棵树\n",
    "            eval_set = [(test_x, test_y)]\n",
    "            if epoch:\n",
    "                bst.load_model('./model.json')\n",
    "                bst = xgb.XGBClassifier(learning_rate=0.01, n_estimators=500, max_depth=6, min_child_weight=3,\n",
    "                              subsample=1, colsample_bytree=1, gamma=0.1, reg_alpha=0.01, reg_lambda=3,objective='reg:logistic')\n",
    "                bst.fit(data, targets, early_stopping_rounds=10, eval_metric=\"error\", eval_set=eval_set, verbose=False)\n",
    "                bst.save_model('./model.json')\n",
    "            else:\n",
    "                bst = xgb.XGBClassifier(learning_rate=0.01, n_estimators=500, max_depth=6, min_child_weight=3,\n",
    "                              subsample=1, colsample_bytree=1, gamma=0.1, reg_alpha=0.01, reg_lambda=3,objective='reg:logistic')\n",
    "                bst.fit(data, targets, early_stopping_rounds=10, eval_metric=\"error\", eval_set=eval_set, verbose=False)\n",
    "                if os.path.exists('./model.json'):\n",
    "                    os.remove('./model.json')\n",
    "                bst.save_model('./model.json')\n",
    "            ypred = torch.Tensor(bst.predict(data))\n",
    "            loss = criterion(ypred,targets)\n",
    "            print(loss)\n",
    "            loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "    model.eval()  # Turn on the evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(test_x)\n",
    "        dtest = xgb.DMatrix(output)\n",
    "        ypred = bst.predict(test_x)\n",
    "        y_pred = ypred\n",
    "        print('Precesion: %.4f' % metrics.precision_score(test_y, y_pred))\n",
    "        print('Recall: %.4f' % metrics.recall_score(test_y, y_pred))\n",
    "        print('F1-score: %.4f' % metrics.f1_score(test_y, y_pred))\n",
    "        print('Accuracy: %.4f' % metrics.accuracy_score(test_y, y_pred))\n",
    "        print('AUC: %.4f' % metrics.roc_auc_score(test_y, ypred))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 30/30 [00:04<00:00,  6.29it/s]\n",
      "Feature Extraction: 100%|██████████| 30/30 [01:17<00:00,  2.59s/it]\n",
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:172: RuntimeWarning:\n",
      "\n",
      "The columns ['open__fft_coefficient__attr_\"real\"__coeff_11'\n",
      " 'open__fft_coefficient__attr_\"real\"__coeff_12'\n",
      " 'open__fft_coefficient__attr_\"real\"__coeff_13' ...\n",
      " 'pre_close__agg_linear_trend__attr_\"stderr\"__chunk_len_50__f_agg_\"mean\"'\n",
      " 'pre_close__agg_linear_trend__attr_\"stderr\"__chunk_len_50__f_agg_\"var\"'\n",
      " 'pre_close__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('data processing...')\n",
    "    settings = EfficientFCParameters()\n",
    "    df = pd.read_csv('./data1.csv', index_col=['bob'])\n",
    "    df.drop(columns=['id', 'position', 'eob', 'frequency'], inplace=True)\n",
    "    df['time'] = df.index\n",
    "\n",
    "    # 滚动处理时间序列，指定id列和时间列，滚动处理时序数据\n",
    "    df_rolled = roll_time_series(df, column_id=\"symbol\", column_sort=\"time\",\n",
    "                                 max_timeshift=20, min_timeshift=5)\n",
    "\n",
    "    df_rolled.drop(columns=['symbol'], inplace=True)  # 删除name列, inplace为直接修改df，否则需要赋值给新的变量\n",
    "\n",
    "    X = extract_features(df_rolled, column_id=\"id\", column_sort=\"time\",\n",
    "                         default_fc_parameters=settings)\n",
    "    impute(X)\n",
    "\n",
    "    X = X.set_index([X.index.map(lambda x: x[0]), X.index.map(lambda x: x[1])], drop=False)\n",
    "    X.index.names = [\"Symbols\", \"last_date\"]\n",
    "\n",
    "    y = df.groupby(\"symbol\").apply(lambda x: x.set_index(\"time\")[\"close\"].shift(-1)).T.unstack().fillna(method='ffill')\n",
    "\n",
    "    y = y[y.index.isin(X.index)]\n",
    "    X = X[X.index.isin(y.index)]\n",
    "\n",
    "    features_filtered_0 = select_features(X, y)\n",
    "    y = df.groupby(\"symbol\").apply(\n",
    "        lambda x: x.set_index(\"time\")[\"close\"].shift(-1).pct_change().fillna(0)).T.unstack().fillna(method='ffill')\n",
    "    y = y.apply(lambda x: True if x > 0 else False)[y.index.isin(features_filtered_0.index)]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "tensor(0.2344)\n",
      "tensor(0.2188)\n",
      "tensor(0.1410)\n",
      "Precesion: 0.5641\n",
      "Recall: 0.5641\n",
      "F1-score: 0.5641\n",
      "Accuracy: 0.5723\n",
      "AUC: 0.5722\n"
     ]
    }
   ],
   "source": [
    "xgboot_reg(features_filtered_0, y)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}